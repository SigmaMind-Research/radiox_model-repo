{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-05T15:20:36.609970Z","iopub.status.busy":"2023-08-05T15:20:36.609123Z","iopub.status.idle":"2023-08-05T15:20:48.208942Z","shell.execute_reply":"2023-08-05T15:20:48.207824Z","shell.execute_reply.started":"2023-08-05T15:20:36.609935Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T06:10:39.703958Z","iopub.status.busy":"2023-08-07T06:10:39.703162Z","iopub.status.idle":"2023-08-07T06:10:39.710080Z","shell.execute_reply":"2023-08-07T06:10:39.709107Z","shell.execute_reply.started":"2023-08-07T06:10:39.703915Z"},"trusted":true},"outputs":[],"source":["# os.makedirs('/kaggle/working/checkpoints/')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:42:57.918622Z","iopub.status.busy":"2023-08-18T16:42:57.918173Z","iopub.status.idle":"2023-08-18T16:43:01.602939Z","shell.execute_reply":"2023-08-18T16:43:01.601858Z","shell.execute_reply.started":"2023-08-18T16:42:57.918579Z"},"trusted":true},"outputs":[],"source":["\n","# --- Base packages ---\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as metrics\n","\n","# --- PyTorch packages ---\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","# --- Helper packages ---\n","from random import shuffle\n","import sentencepiece as spm\n","from PIL import Image, ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","# --- Datasets ---\n","class NLMCXR(data.Dataset): # Open-I Dataset\n","    def __init__(self, directory, input_size=(256,256), random_transform=True,\n","                view_pos=['AP', 'PA', 'LATERAL'], max_views=2, sources=['image','history'], targets=['label'], \n","                max_len=1000, vocab_file='report-generation-support/nlmcxr_unigram_1000.model'):\n","        \n","        self.source_sections = ['INDICATION', 'COMPARISON']\n","        self.target_sections = ['FINDINGS']\n","        self.vocab = spm.SentencePieceProcessor(model_file=directory + vocab_file)\n","        self.vocab_file = vocab_file # Save it for subsets\n","\n","        self.sources = sources # Choose which section as input\n","        self.targets = targets # Choose which section as output\n","        self.max_views = max_views\n","        self.view_pos = view_pos\n","        self.max_len = max_len\n","\n","        self.dir = directory\n","        self.input_size = input_size\n","        self.random_transform = random_transform\n","        self.__input_data(binary_mode=True)\n","        \n","        if random_transform:\n","            self.transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(),\n","                transforms.RandomApply([\n","                    transforms.ColorJitter(0.1,0.1,0.1), \n","                    transforms.RandomRotation(15, expand=True)]),\n","                transforms.Resize(input_size),\n","                transforms.ToTensor(),\n","            ])\n","        else:\n","            self.transform = transforms.Compose([transforms.Resize(input_size), transforms.ToTensor()])\n","    \n","    def __len__(self):\n","        return len(self.file_list)\n","\n","    def __getitem__(self, idx):\n","        file_name = self.file_list[idx]\n","        sources, targets = [], []\n","        tmp_rep = self.captions[self.file_report[file_name]['image'][0] + '.png']\n","        \n","        # ------ Multiview Images ------\n","        if 'image' in self.sources:\n","            imgs, vpos = [], []\n","            images = self.file_report[file_name]['image']\n","            # Randomly select V images from each folder \n","            new_orders = np.random.permutation(len(images))\n","            img_files = np.array(images)[new_orders].tolist()\n","#             print(images)\n","            for i in range(min(self.max_views,len(img_files))):\n","                if img_files[i] not in ['CXR1_1_IM-0001-4001','CXR1_1_IM-0001-3001']:\n","                    img_file = os.path.join(self.dir, 'chest-xrays-indiana-university', 'images', 'images_normalized', img_files[i][3:] + '.dcm.png')\n","                else:\n","                    img_file = os.path.join(self.dir, 'chest-xrays-indiana-university', 'images', 'images_normalized', img_files[i][5:] + '.dcm.png')\n","                    #self.dir + img_files[i] + '.png'\n","                img = Image.open(img_file).convert('RGB')\n","                imgs.append(self.transform(img).unsqueeze(0)) # (1,C,W,H)\n","                vpos.append(1) # We do not know what view position of the image is, so just let it be 1\n","                \n","            # If the number of images is smaller than V, pad the tensor with dummy images\n","            cur_len = len(vpos)\n","            for i in range(cur_len, self.max_views):\n","                imgs.append(torch.zeros_like(imgs[0]))\n","                vpos.append(-1) # Empty mask\n","            \n","            imgs = torch.cat(imgs, dim=0) # (V,C,W,H)\n","            vpos = np.array(vpos, dtype=np.int64) # (V)\n","\n","        # ------ Additional Information ------\n","        info = self.file_report[file_name]['report']\n","        \n","        source_info = []\n","        for section, content in info.items():\n","            if section in self.source_sections:\n","                source_info.append(content)\n","        source_info = ' '.join(source_info)\n","        \n","        encoded_source_info = [self.vocab.bos_id()] + self.vocab.encode(source_info) + [self.vocab.eos_id()]\n","        source_info = np.ones(self.max_len, dtype=np.int64) * self.vocab.pad_id()\n","        source_info[:min(len(encoded_source_info), self.max_len)] = encoded_source_info[:min(len(encoded_source_info), self.max_len)]\n","\n","        target_info = []\n","        for section, content in info.items():\n","            if section in self.target_sections:\n","                target_info.append(content)\n","        # target_info = ' '.join(target_info)\n","        target_info = tmp_rep # This load the document from our previous AAAI paper (preprocessed documents)\n","        \n","        np_labels = np.zeros(len(self.top_np), dtype=float)\n","        for i in range(len(self.top_np)):\n","            if self.top_np[i] in target_info:\n","                np_labels[i] = 1\n","        \n","        encoded_target_info = [self.vocab.bos_id()] + self.vocab.encode(target_info) + [self.vocab.eos_id()]\n","        target_info = np.ones(self.max_len, dtype=np.int64) * self.vocab.pad_id()\n","        target_info[:min(len(encoded_target_info), self.max_len)] = encoded_target_info[:min(len(encoded_target_info), self.max_len)]\n","\n","        for i in range(len(self.sources)):\n","            if self.sources[i] == 'image':\n","                sources.append((imgs,vpos))\n","            if self.sources[i] == 'history':\n","                sources.append(source_info)\n","            if self.sources[i] == 'label':\n","                sources.append(np.concatenate([np.array(self.file_labels[file_name]), np_labels]))\n","            if self.sources[i] == 'caption':\n","                sources.append(target_info)\n","            if self.sources[i] == 'caption_length':\n","                sources.append(min(len(encoded_target_info), self.max_len))\n","                \n","        for i in range(len(self.targets)):\n","            if self.targets[i] == 'label':\n","                targets.append(np.concatenate([np.array(self.file_labels[file_name]), np_labels]))\n","            if self.targets[i] == 'caption':\n","                targets.append(target_info)\n","            if self.targets[i] == 'caption_length':\n","                targets.append(min(len(encoded_target_info), self.max_len))\n","                \n","        return sources if len(sources) > 1 else sources[0], targets if len(targets) > 1 else targets[0]\n","\n","    def __get_nounphrase(self, top_k=100, file_name='report-generation-support/count_nounphrase.json'):\n","        count_np = json.load(open(self.dir + file_name, 'r'))\n","        sorted_count_np = sorted([(k,v) for k,v in count_np.items()], key=lambda x: x[1], reverse=True)\n","        top_nounphrases = [k for k,v in sorted_count_np][:top_k]\n","        return top_nounphrases\n","\n","    def __input_data(self, binary_mode=True):\n","        self.__input_caption()\n","        self.__input_report()\n","        self.__input_label()\n","        self.__filter_inputs()\n","        self.top_np = self.__get_nounphrase()\n","        \n","    def __input_label(self):\n","        with open(self.dir + 'report-generation-support/file2label.json') as f:\n","            labels = json.load(f)\n","        self.file_labels = labels\n","        \n","    def __input_caption(self):\n","        with open(self.dir + 'report-generation-support/captions.json') as f:\n","            captions = json.load(f)\n","        self.captions = captions\n","        \n","    def __input_report(self):\n","        with open(self.dir + 'report-generation-support/reports_ori.json') as f:\n","            reports = json.load(f)\n","        self.file_list = [k for k in reports.keys()]\n","        self.file_report = reports\n","\n","    def __filter_inputs(self):\n","        filtered_file_report = {}\n","        for k, v in self.file_report.items():\n","            if (len(v['image']) > 0) and (('FINDINGS' in v['report']) and (v['report']['FINDINGS'] != '')): # or (('IMPRESSION' in v['report']) and (v['report']['IMPRESSION'] != ''))):\n","                filtered_file_report[k] = v\n","        self.file_report = filtered_file_report\n","        self.file_list = [k for k in self.file_report.keys()]\n","\n","    def get_subsets(self, train_size=0.7, val_size=0.1, test_size=0.2, seed=0):\n","        np.random.seed(seed)\n","        indices = np.random.permutation(len(self.file_list))\n","        train_pvt = int(train_size * len(self.file_list))\n","        val_pvt = int((train_size + val_size) * len(self.file_list))\n","        train_indices = indices[:train_pvt]\n","        val_indices = indices[train_pvt:val_pvt]\n","        test_indices = indices[val_pvt:]\n","\n","        master_file_list = np.array(self.file_list)\n","\n","        train_dataset = NLMCXR(self.dir, self.input_size, self.random_transform, \n","                              self.view_pos, self.max_views, self.sources, self.targets, self.max_len, self.vocab_file)\n","        train_dataset.file_list = master_file_list[train_indices].tolist()\n","\n","        # Consider change random_transform to False for validation\n","        val_dataset = NLMCXR(self.dir, self.input_size, False, \n","                            self.view_pos, self.max_views, self.sources, self.targets, self.max_len, self.vocab_file)\n","        val_dataset.file_list = master_file_list[val_indices].tolist()\n","\n","        # Consider change random_transform to False for testing\n","        test_dataset = NLMCXR(self.dir, self.input_size, False, \n","                             self.view_pos, self.max_views, self.sources, self.targets, self.max_len, self.vocab_file)\n","        test_dataset.file_list = master_file_list[test_indices].tolist()\n","\n","        return train_dataset, val_dataset, test_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:01.605527Z","iopub.status.busy":"2023-08-18T16:43:01.604943Z","iopub.status.idle":"2023-08-18T16:43:01.626757Z","shell.execute_reply":"2023-08-18T16:43:01.625778Z","shell.execute_reply.started":"2023-08-18T16:43:01.605494Z"},"trusted":true},"outputs":[],"source":["\n","\n","# ------ Helper Functions ------\n","def data_to_device(data, device='cpu'):\n","\tif isinstance(data, torch.Tensor):\n","\t\tdata = data.to(device)\n","\telif isinstance(data, tuple):\n","\t\tdata = tuple(data_to_device(item,device) for item in data)\n","\telif isinstance(data, list):\n","\t\tdata = list(data_to_device(item,device) for item in data)\n","\telif isinstance(data, dict):\n","\t\tdata = dict((k,data_to_device(v,device)) for k,v in data.items())\n","\telse:\n","\t\traise TypeError('Unsupported Datatype! Must be a Tensor/List/Tuple/Dict.')\n","\treturn data\n","\n","def data_concatenate(iterable_data, dim=0):\n","\tdata = iterable_data[0] # can be a list / tuple / dict / tensor\n","\tif isinstance(data, torch.Tensor):\n","\t\treturn torch.cat([*iterable_data], dim=dim)\n","\telif isinstance(data, tuple):\n","\t\tnum_cols = len(data)\n","\t\tnum_rows = len(iterable_data)\n","\t\treturn_data = []\n","\t\tfor col in range(num_cols):\n","\t\t\tdata_col = []\n","\t\t\tfor row in range(num_rows):\n","\t\t\t\tdata_col.append(iterable_data[row][col])\n","\t\t\treturn_data.append(torch.cat([*data_col], dim=dim))\n","\t\treturn tuple(return_data)\n","\telif isinstance(data, list):\n","\t\tnum_cols = len(data)\n","\t\tnum_rows = len(iterable_data)\n","\t\treturn_data = []\n","\t\tfor col in range(num_cols):\n","\t\t\tdata_col = []\n","\t\t\tfor row in range(num_rows):\n","\t\t\t\tdata_col.append(iterable_data[row][col])\n","\t\t\treturn_data.append(torch.cat([*data_col], dim=dim))\n","\t\treturn list(return_data)\n","\telif isinstance(data, dict):\n","\t\tnum_cols = len(data)\n","\t\tnum_rows = len(iterable_data)\n","\t\treturn_data = []\n","\t\tfor col in data.keys():\n","\t\t\tdata_col = []\n","\t\t\tfor row in range(num_rows):\n","\t\t\t\tdata_col.append(iterable_data[row][col])\n","\t\t\treturn_data.append(torch.cat([*data_col], dim=dim))\n","\t\treturn dict((k,return_data[i]) for i,k in enumerate(data.keys()))\n","\telse:\n","\t\traise TypeError('Unsupported Datatype! Must be a Tensor/List/Tuple/Dict.')\n","\n","def data_distributor(model, source):\n","\tif isinstance(source, torch.Tensor):\n","\t\toutput = model(source)\n","\telif isinstance(source, tuple) or isinstance(source, list):\n","\t\toutput = model(*source)\n","\telif isinstance(source, dict):\n","\t\toutput = model(**source)\n","\telse:\n","\t\traise TypeError('Unsupported DataType! Try List/Tuple!')\n","\treturn output\n","\t\n","def args_to_kwargs(args, kwargs_list=None): # This function helps distribute input to corresponding arguments in Torch models\n","\tif kwargs_list != None:\n","\t\tif isinstance(args, dict): # Nothing to do here\n","\t\t\treturn args \n","\t\telse: # args is a list or tuple or single element\n","\t\t\tif isinstance(args, torch.Tensor): # single element\n","\t\t\t\targs = [args]\n","\t\t\tassert len(args) == len(kwargs_list)\n","\t\t\treturn dict(zip(kwargs_list, args))\n","\telse: # Nothing to do here\n","\t\treturn args"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:01.792290Z","iopub.status.busy":"2023-08-18T16:43:01.791904Z","iopub.status.idle":"2023-08-18T16:43:01.807186Z","shell.execute_reply":"2023-08-18T16:43:01.805853Z","shell.execute_reply.started":"2023-08-18T16:43:01.792260Z"},"trusted":true},"outputs":[],"source":["class TextDataset(data.Dataset):\n","    def __init__(self, text_file, label_file, sources=['caption'], targets=['label'],\n","                 vocab_file='/kaggle/input/report-generation-support/nlmcxr_unigram_1000.model', max_len=1000):\n","        self.text_file = text_file\n","        self.label_file = label_file\n","        self.vocab = spm.SentencePieceProcessor(model_file=vocab_file)\n","        self.sources = sources # Choose which section as input\n","        self.targets = targets # Choose which section as output\n","        self.max_len = max_len\n","        self.__input_data()\n","\n","    def __len__(self):\n","        return len(self.lines)\n","\n","    def __getitem__(self, idx):\n","        encoded_text = [self.vocab.bos_id()] + self.vocab.encode(self.lines[idx].strip()) + [self.vocab.eos_id()]\n","        text = np.ones(self.max_len, dtype=np.int64) * self.vocab.pad_id()\n","        text[:min(len(encoded_text), self.max_len)] = encoded_text[:min(len(encoded_text), self.max_len)]\n","        \n","        sources = []\n","        for i in range(len(self.sources)):\n","            if self.sources[i] == 'label':\n","                sources.append(self.labels[idx])\n","            if self.sources[i] == 'caption':\n","                sources.append(text)\n","            if self.sources[i] == 'caption_length':\n","                sources.append(min(len(encoded_text), self.max_len))\n","        \n","        targets = []\n","        for i in range(len(self.targets)):\n","            if self.targets[i] == 'label':\n","                targets.append(self.labels[idx])\n","            if self.targets[i] == 'caption':\n","                targets.append(text)\n","            if self.targets[i] == 'caption_length':\n","                targets.append(min(len(encoded_text), self.max_len))\n","                \n","        return sources if len(sources) > 1 else sources[0], targets if len(targets) > 1 else targets[0]\n","    \n","    def __input_data(self):\n","        data_file = open(self.text_file, 'r') \n","        self.lines = data_file.readlines()\n","        self.labels = np.loadtxt(self.label_file, dtype='float')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:02.477145Z","iopub.status.busy":"2023-08-18T16:43:02.476691Z","iopub.status.idle":"2023-08-18T16:43:02.503002Z","shell.execute_reply":"2023-08-18T16:43:02.501634Z","shell.execute_reply.started":"2023-08-18T16:43:02.477113Z"},"trusted":true},"outputs":[],"source":["# ------ Core Functions ------\n","def train(data_loader, model, optimizer, criterion, scheduler=None, device='cpu', kw_src=None, kw_tgt=None, kw_out=None, scaler=None):\n","\tmodel.train()\n","\trunning_loss = 0\n"," \n","\tprog_bar = tqdm(data_loader)\n","\tfor i, (source, target) in enumerate(prog_bar):\n","\t\tsource = data_to_device(source, device)\n","\t\ttarget = data_to_device(target, device)\n","\n","\t\tsource = args_to_kwargs(source, kw_src)\n","\t\ttarget = args_to_kwargs(target, kw_tgt)\n","\n","\t\tif scaler != None:\n","\t\t\twith torch.cuda.amp.autocast():\n","\t\t\t\toutput = data_distributor(model, source)\n","\t\t\t\toutput = args_to_kwargs(output, kw_out)\n","\t\t\t\tloss = criterion(output, target)\n","\t\t\t\t\n","\t\t\trunning_loss += loss.item()\n","\t\t\tprog_bar.set_description('Loss: {}'.format(running_loss/(i+1)))\n","\n","\t\t\t# Back-propagate and update weights\n","\t\t\toptimizer.zero_grad()\n","\t\t\tscaler.scale(loss).backward()\n","\t\t\tscaler.step(optimizer)\n","\t\t\tscaler.update()\n","\t\t\tif scheduler != None:\n","\t\t\t\tscheduler.step()\n","\t\telse:\n","\t\t\toutput = data_distributor(model, source)\n","\t\t\toutput = args_to_kwargs(output, kw_out)\n","\t\t\tloss = criterion(output, target)\n","\n","\t\t\trunning_loss += loss.item()\n","\t\t\tprog_bar.set_description('Loss: {}'.format(running_loss/(i+1)))\n","\n","\t\t\t# Back-propagate and update weights\n","\t\t\toptimizer.zero_grad()\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\t\t\tif scheduler != None:\n","\t\t\t\tscheduler.step()\n","\n","\treturn running_loss / len(data_loader)\n","\n","def test(data_loader, model, criterion=None, device='cpu', return_results=True, kw_src=None, kw_tgt=None, kw_out=None, select_outputs=[]):\n","\tmodel.eval()\n","\trunning_loss = 0\n","\n","\toutputs = []\n","\ttargets = []\n","\n","\twith torch.no_grad():\n","\t\tprog_bar = tqdm(data_loader)\n","\t\tfor i, (source, target) in enumerate(prog_bar):\n","\t\t\tsource = data_to_device(source, device)\n","\t\t\ttarget = data_to_device(target, device)\n","\n","\t\t\tsource = args_to_kwargs(source, kw_src)\n","\t\t\ttarget = args_to_kwargs(target, kw_tgt)\n","\n","\t\t\toutput = data_distributor(model, source)\n","\t\t\toutput = args_to_kwargs(output, kw_out)\n","\n","\t\t\tif criterion != None:\n","\t\t\t\tloss = criterion(output, target)\n","\t\t\t\trunning_loss += loss.item()\n","\t\t\tprog_bar.set_description('Loss: {}'.format(running_loss/(i+1)))\n","\n","\t\t\tif return_results:\n","\t\t\t\tif len(select_outputs) == 0:\n","\t\t\t\t\toutputs.append(data_to_device(output,'cpu'))\n","\t\t\t\t\ttargets.append(data_to_device(target,'cpu'))\n","\t\t\t\telse:\n","\t\t\t\t\tlist_output = [output[row] for row in select_outputs]\n","\t\t\t\t\tlist_target = [target[row] for row in select_outputs]\n","\t\t\t\t\toutputs.append(data_to_device(list_output if len(list_output) > 1 else list_output[0],'cpu'))\n","\t\t\t\t\ttargets.append(data_to_device(list_target if len(list_target) > 1 else list_target[0],'cpu'))\n","\t\n","\tif return_results:\n","\t\toutputs = data_concatenate(outputs)\n","\t\ttargets = data_concatenate(targets)\n","\t\treturn running_loss / len(data_loader), outputs, targets\n","\telse:\n","\t\treturn running_loss / len(data_loader)\n","\n","def save(path, model, optimizer=None, scheduler=None, epoch=-1, stats=None):\n","\ttorch.save({\n","\t\t# --- Model Statistics ---\n","\t\t'epoch': epoch,\n","\t\t'stats': stats,\n","\t\t# --- Model Parameters ---\n","\t\t'model_state_dict': model.state_dict(),\n","\t\t'optimizer_state_dict': optimizer.state_dict() if optimizer != None else None,\n","\t\t'scheduler_state_dict': scheduler.state_dict() if scheduler != None else None,\n","\t}, path)\n","\n","def load(path, model, optimizer=None, scheduler=None):\n","\tcheckpoint = torch.load(path)\n","\t# --- Model Statistics ---\n","\tepoch = checkpoint['epoch']\n","\tstats = checkpoint['stats']\n","\t# --- Model Parameters ---\n","\tmodel.load_state_dict(checkpoint['model_state_dict'])\n","\tif optimizer != None:\n","\t\ttry:\n","\t\t\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\t\texcept: # Input optimizer doesn't fit the checkpoint one --> should be ignored\n","\t\t\tprint('Cannot load the optimizer')\n","\tif scheduler != None:\n","\t\ttry:\n","\t\t\tscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\t\texcept: # Input scheduler doesn't fit the checkpoint one --> should be ignored\n","\t\t\tprint('Cannot load the scheduler')\n","\treturn epoch, stats"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:03.322534Z","iopub.status.busy":"2023-08-18T16:43:03.321704Z","iopub.status.idle":"2023-08-18T16:43:03.342389Z","shell.execute_reply":"2023-08-18T16:43:03.341326Z","shell.execute_reply.started":"2023-08-18T16:43:03.322490Z"},"trusted":true},"outputs":[],"source":["\n","\n","class KLLoss(nn.Module):\n","\tdef __init__(self):\n","\t\tsuper().__init__()\n","\t\tself.KLLoss = nn.KLDivLoss()\n","\n","\tdef forward(self, output, target):\n","\t\t'''\n","\t\tOutput: (N,*) \\n\n","\t\tTarget: (N,*) \\n\n","\t\t'''\n","\t\toutput = torch.log(output)  # Invert softmax\n","\t\t# target = torch.log(target) # Invert softmax\n","\t\t# How output distribution differs from target distribution\n","\t\treturn self.KLLoss(output, target)\n","\n","\n","class CELoss(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n","\n","\tdef forward(self, output, target):\n","\t\t'''\n","\t\tOutput: (N,*,C) \\n\n","\t\tTarget: (N,*) \\n\n","\t\t'''\n","\t\toutput = torch.log(output)  # Invert softmax\n","\t\toutput = output.reshape(-1, output.shape[-1])  # (*,C)\n","\t\ttarget = target.reshape(-1).long()  # (*)\n","\t\treturn self.CELoss(output, target)\n","\n","\n","class CELossSame(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n","\n","\tdef forward(self, outputs, target):\n","\t\t'''\n","\t\tOutput: (N,*,C) \\n\n","\t\tTarget: (N,*) \\n\n","\t\t'''\n","\t\toutput_img = torch.log(outputs[0]) # Invert softmax\n","\t\toutput_txt = torch.log(outputs[1])\n","\t\toutput_sen = torch.log(outputs[2])\n","\n","\t\toutput_img = output_img.reshape(-1, output_img.shape[-1]) # (*,C)\n","\t\toutput_txt = output_txt.reshape(-1, output_txt.shape[-1]) # (*,C)\n","\t\toutput_sen = output_sen.reshape(-1, output_sen.shape[-1]) # (*,C)\n","\t\ttarget = target.reshape(-1).long() # (*)\n","\t\treturn self.CELoss(output_img, target) + self.CELoss(output_txt, target) + self.CELoss(output_sen, target)\n","\n","class CELossShift(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = CELoss(ignore_index=ignore_index)\n","\n","\tdef forward(self, output, target):\n","\t\t'''\n","\t\tOutput: (N,*,C) \\n\n","\t\tTarget: (N,*) \\n\n","\t\t'''\n","\t\toutput = output[:,:-1,:] # (* - 1,C)\n","\t\ttarget = target[:,1:] # (* - 1)\n","\t\treturn self.CELoss(output, target)\n","\n","class CELossTotal(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = CELoss()\n","\t\tself.CELossShift = CELossShift(ignore_index=ignore_index)\n","\n","\tdef forward(self, output, target):\n","\t\treturn self.CELossShift(output[0], target[0]) + self.CELoss(output[1], target[1])\n","\n","class CELossTotalEval(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = CELoss()\n","\t\tself.CELossShift = CELossShift(ignore_index=ignore_index)\n","\n","\tdef forward(self, output, target):\n","\t\treturn self.CELossShift(output[0], target[0]) + self.CELoss(output[1], target[1]) + self.CELoss(output[2], target[1])\n","\n","class CELossTransfer(nn.Module):\n","\tdef __init__(self, ignore_index=-1):\n","\t\tsuper().__init__()\n","\t\tself.CELoss = CELoss()\n","\t\tself.CELossShift = CELossShift(ignore_index=ignore_index)\n","\n","\tdef forward(self, output, target):\n","\t\treturn self.CELossShift(output[0], target[0]) # + self.CELoss(output[1], target[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:04.006079Z","iopub.status.busy":"2023-08-18T16:43:04.005322Z","iopub.status.idle":"2023-08-18T16:43:04.063954Z","shell.execute_reply":"2023-08-18T16:43:04.062834Z","shell.execute_reply.started":"2023-08-18T16:43:04.006044Z"},"trusted":true},"outputs":[],"source":["\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class Transformer(nn.Module):\n","    def __init__(self, image_encoder, num_tokens, num_posits, fc_features=1024, embed_dim=256, num_heads=8, fwd_dim=4096, dropout=0.1, num_layers_enc=1, num_layers_dec=6, freeze_encoder=True):\n","\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(num_tokens, embed_dim)\n","        self.posit_embedding = nn.Embedding(num_posits, embed_dim)\n","        self.pixel_embedding = nn.Embedding(64, embed_dim) # last convolution layer has 8x8 pixels = 64 pixels\n","        \n","        self.transformer_enc = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(embed_dim,num_heads,fwd_dim,dropout), num_layers=num_layers_enc)\n","        self.transformer_dec = nn.TransformerDecoder(decoder_layer=nn.TransformerDecoderLayer(embed_dim,num_heads,fwd_dim,dropout), num_layers=num_layers_dec)\n","        \n","        self.fc1 = nn.Linear(fc_features, embed_dim)\n","        self.fc2 = nn.Linear(embed_dim, num_tokens)\n","        \n","        self.image_encoder = image_encoder # make sure that image_encoder is a MVCNN model\n","        if freeze_encoder: # The orginal paper freeze the densenet which is pretrained on ImageNet. Suprisingly, the results were very good\n","            for param in self.image_encoder.parameters():\n","                param.requires_grad = False\n","                \n","        self.dropout = nn.Dropout(dropout)\n","        self.num_tokens = num_tokens\n","        self.num_posits = num_posits\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","    \n","    def forward(self, image, caption = None, bos_id=1, eos_id=2, pad_id=3, max_len=300):\n","        if caption != None:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            wxh_features = wxh_features.view(wxh_features.shape[0], wxh_features.shape[1], -1).permute(0,2,1) # (B,W*H,F)\n","            wxh_features = self.fc1(wxh_features) # (B,W*H,E)\n","\n","            pixel = torch.arange(wxh_features.shape[1]).unsqueeze(0).repeat(wxh_features.shape[0],1).to(wxh_features.device)\n","            pixel_embed = self.pixel_embedding(pixel) # (B,W*H,E)\n","            img_features = wxh_features + pixel_embed # (B,W*H,E)\n","            img_features = self.transformer_enc(img_features.transpose(0,1)).transpose(0,1) # (B,W*H,E)\n","            \n","            posit = torch.arange(caption.shape[1]).unsqueeze(0).repeat(caption.shape[0],1).to(caption.device) # (1,L) --> (B,L)\n","            posit_embed = self.posit_embedding(posit) # (B,L,E)\n","            token_embed = self.token_embedding(caption) # (B,L,E)\n","            cap_features = token_embed + posit_embed # (B,L,E)\n","            \n","            tgt_mask = self.generate_square_subsequent_mask(caption.shape[1]).to(caption.device)\n","            output = self.transformer_dec(tgt=cap_features.transpose(0,1), \n","                                          memory=img_features.transpose(0,1),\n","                                          tgt_mask=tgt_mask,\n","                                          tgt_key_padding_mask=(caption == pad_id)).transpose(0, 1) # (L,B,E) -> (B,L,E)\n","            \n","            preds = self.fc2(self.dropout(output)) # (B,L,S)\n","            preds = torch.softmax(preds, dim = -1) # (B,L,S)\n","            return preds # (B,L,S)\n","\n","        else:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            wxh_features = wxh_features.view(wxh_features.shape[0], wxh_features.shape[1], -1).permute(0,2,1) # (B,W*H,F)\n","            wxh_features = self.fc1(wxh_features) # (B,W*H,E)\n","            \n","            pixel = torch.arange(wxh_features.shape[1]).unsqueeze(0).repeat(wxh_features.shape[0],1).to(wxh_features.device)\n","            pixel_embed = self.pixel_embedding(pixel) # (B,W*H,E)\n","            img_features = wxh_features + pixel_embed # (B,W*H,E)\n","            img_features = self.transformer_enc(img_features.transpose(0,1)).transpose(0,1) # (B,W*H,E)\n","            \n","            caption = torch.ones((img_features.shape[0],1), dtype=torch.long).to(img_features.device) * bos_id # (B,1)\n","            for _ in range(max_len):\n","                posit = torch.arange(caption.shape[1]).unsqueeze(0).repeat(caption.shape[0],1).to(caption.device) # (1,L') --> (B,L')\n","                posit_embed = self.posit_embedding(posit) # (B,L',E)\n","                token_embed = self.token_embedding(caption) # (B,L',E)\n","                cap_features = token_embed + posit_embed # (B,L',E)\n","\n","                tgt_mask = self.generate_square_subsequent_mask(caption.shape[1]).to(caption.device)\n","                output = self.transformer_dec(tgt=cap_features.transpose(0,1), \n","                                              memory=img_features.transpose(0,1),\n","                                              tgt_mask=tgt_mask,\n","                                              tgt_key_padding_mask=(caption == pad_id)).transpose(0, 1) # (L',B,E) -> (B,L',E)\n","                \n","                preds = self.fc2(self.dropout(output)) # (B,L',S)\n","                preds = torch.softmax(preds, dim = -1) # (B,L',S)\n","                preds = torch.argmax(preds[:,-1,:], dim=-1, keepdim=True) # (B,1)\n","                caption = torch.cat([caption, preds], dim=-1) # (B,L'+1)\n","            \n","            return caption # (B,L')\n","        \n","class GumbelTransformer(nn.Module):\n","    def __init__(self, transformer, diff_chexpert, freeze_chexpert=True):\n","        super().__init__()\n","        self.transformer = transformer\n","        self.diff_chexpert = diff_chexpert\n","        if freeze_chexpert:\n","            for param in self.diff_chexpert.parameters():\n","                param.requires_grad = False\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","    \n","    def apply_chexpert(self, embed, caption_length):\n","        padding_mask = self.diff_chexpert.generate_pad_mask(embed.shape[0], embed.shape[1], caption_length)\n","        output, (_, _) = self.diff_chexpert.rnn(embed)\n","        y_hats = [attn(output, padding_mask) for attn in self.diff_chexpert.attns]\n","        y_hats = torch.stack(y_hats, dim=1)\n","        y_hats = torch.softmax(y_hats, dim=-1)\n","        return y_hats\n","        \n","    def forward(self, image, caption = None, caption_length=None, bos_id=1, eos_id=2, pad_id=3, max_len=300, temperature=1, beta=1):\n","        if caption != None:\n","            preds = self.transformer(image, caption, bos_id, eos_id, pad_id, max_len) # (B,L,S)\n","            \n","            logits = torch.log(preds) # (B,L,S)            \n","            one_hot_preds = self.gumbel_softmax_sample(logits,temperature,beta) # (B,L,S)\n","            vocab = torch.arange(self.transformer.num_tokens).unsqueeze(0).repeat(caption.shape[0],1).to(caption.device) # (1,S) --> (B,S)\n","            vocab_embed = self.transformer.token_embedding(vocab) # (B,S,E)\n","            preds_embed = one_hot_preds @ vocab_embed # (B,L,S) x (B,S,E) = (B,L,E)\n","            chexpert_preds = self.apply_chexpert(preds_embed, caption_length) # (B,D,C)\n","            return preds, chexpert_preds # (B,L,S), (B,D,C)\n","\n","        else:\n","            caption = self.transformer(image, caption, bos_id, eos_id, pad_id, max_len) # (B,L')\n","            return caption # (B,L')\n","        \n","    def sample_gumbel(self, shape, device, eps=1e-20):\n","        U = torch.rand(shape).to(device)\n","        return -torch.log(-torch.log(U + eps) + eps)\n","    \n","    def gumbel_softmax_sample(self, logits, temperature, beta):\n","        y = logits + beta * self.sample_gumbel(logits.size(), logits.device)\n","        return torch.softmax(y / temperature, dim=-1)\n","    \n","# --- CheXpert ---\n","class TanhAttention(nn.Module):\n","    def __init__(self, hidden_size, dropout=0.5, num_out=2):\n","        super(TanhAttention, self).__init__()\n","        self.attn1 = nn.Linear(hidden_size, hidden_size // 2)\n","        self.attn2 = nn.Linear(hidden_size // 2, 1, bias=False)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.fc = nn.Linear(hidden_size, num_out)\n","\n","    def forward(self, output, mask):\n","        attn1 = nn.Tanh()(self.attn1(output))\n","        attn2 = self.attn2(attn1).squeeze(-1)\n","        attn = F.softmax(torch.add(attn2, mask), dim=1)\n","\n","        h = output.transpose(1, 2).matmul(attn.unsqueeze(2)).squeeze(2)\n","        y_hat = self.fc(self.dropout(h))\n","        \n","        return y_hat\n","\n","class DotAttention(nn.Module):\n","    def __init__(self, hidden_size, dropout=0.5, num_out=2):\n","        super(DotAttention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.attn = nn.Linear(hidden_size, 1, bias=False)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.fc = nn.Linear(hidden_size, num_out)\n","\n","    def forward(self, output, mask):\n","        attn = (self.attn(output) / (self.hidden_size ** 0.5)).squeeze(-1)\n","        attn = F.softmax(torch.add(attn, mask), dim=1)\n","\n","        h = output.transpose(1, 2).matmul(attn.unsqueeze(2)).squeeze(2)\n","        y_hat = self.fc(self.dropout(h))\n","\n","        return y_hat\n","    \n","class LSTM_Attn(nn.Module):\n","    def __init__(self, num_tokens, embed_dim, hidden_size, num_topics, num_states, dropout=0.1):\n","        super().__init__()\n","        self.embed = nn.Embedding(num_tokens, embed_dim)\n","        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_size, batch_first=True, bidirectional=True)\n","        self.attns = nn.ModuleList([TanhAttention(hidden_size*2, dropout, num_states) for i in range(num_topics)])\n","\n","    def generate_pad_mask(self, batch_size, max_len, caption_length):\n","        mask = torch.full((batch_size, max_len), fill_value=float('-inf'), dtype=torch.float, device='cuda')\n","        for ind, cap_len in enumerate(caption_length):\n","            mask[ind][:cap_len] = 0\n","        return mask\n","\n","    def forward(self, caption, caption_length):\n","        x = self.embed(caption) # (B,L,E)\n","        output, (_,_) = self.rnn(x)\n","\n","        padding_mask = self.generate_pad_mask(caption.shape[0], caption.shape[1], caption_length)\n","\n","        y_hats = [attn(output, padding_mask) for attn in self.attns]\n","        y_hats = torch.stack(y_hats, dim=1)\n","        y_hats = torch.softmax(y_hats, dim=-1)\n","        return y_hats\n","\n","class CNN_Attn(nn.Module):\n","    def __init__(self, embed_weight, emb_dim, filters, kernels, num_classes=14):\n","\n","        super(CNN_Attn, self).__init__()\n","\n","        self.embed = nn.Embedding.from_pretrained(torch.from_numpy(embed_weight), freeze=True)\n","\n","        self.Ks = kernels\n","\n","        self.convs = nn.ModuleList([nn.Conv1d(emb_dim, filters, K) for K in self.Ks])\n","\n","        self.attns = nn.ModuleList([DotAttention(filters) for _ in range(num_classes)])\n","\n","    def generate_pad_mask(self, batch_size, max_len, caption_length):\n","        total_len = max_len*len(self.Ks)\n","        for K in self.Ks:\n","            total_len -= (K-1)\n","        mask = torch.full((batch_size, total_len), fill_value=float('-inf'), dtype=torch.float, device='cuda')\n","        for ind1, cap_len in enumerate(caption_length):\n","            for ind2, K in enumerate(self.Ks):\n","                mask[ind1][max_len*ind2:cap_len-(K-1)] = 0\n","\n","        return mask\n","\n","    def forward(self, encoded_captions, caption_length):\n","        x = self.embed(encoded_captions).transpose(1, 2)\n","\n","        batch_size = encoded_captions.size(0)\n","        max_len = encoded_captions.size(1)\n","        padding_mask = self.generate_pad_mask(batch_size, max_len, caption_length)\n","\n","        output = [F.relu(conv(x)).transpose(1, 2) for conv in self.convs]\n","        output = torch.cat(output, dim=1)\n","\n","\n","        y_hats = [attn(output, padding_mask) for attn in self.attns]\n","        y_hats = torch.stack(y_hats, dim=1)\n","\n","        return y_hats"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:04.759522Z","iopub.status.busy":"2023-08-18T16:43:04.758901Z","iopub.status.idle":"2023-08-18T16:43:04.796439Z","shell.execute_reply":"2023-08-18T16:43:04.794411Z","shell.execute_reply.started":"2023-08-18T16:43:04.759472Z"},"trusted":true},"outputs":[],"source":["\n","\n","class ST(nn.Module): # Show and Tell\n","    def __init__(self, image_encoder, num_tokens, fc_features=1024, embed_dim=256, hidden_size=512, dropout=0.1, freeze_encoder=True):\n","        super().__init__()\n","        self.embed = nn.Embedding(num_tokens, embed_dim)\n","        \n","        self.image_encoder = image_encoder\n","        if freeze_encoder: # The orginal paper freeze the densenet which is pretrained on ImageNet. Suprisingly, the results were very good\n","            for param in self.image_encoder.parameters():\n","                param.requires_grad = False\n","        \n","        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n","        \n","        self.fc1 = nn.Linear(fc_features, embed_dim)\n","        self.fc2 = nn.Linear(hidden_size, num_tokens)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, image, caption=None, caption_length=None, bos_id=1, eos_id=2, pad_id=3, max_len=300):\n","        if caption != None:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            \n","            img_features = self.fc1(avg_features) # (B,F) --> (B,E)\n","            cap_embed = self.embed(caption) # (B,L,E)\n","            embed = torch.cat([img_features.unsqueeze(1), cap_embed], dim=1) # (B,1+L,E)\n","            \n","            output, _ = self.rnn(embed) # (B,1+L,H)\n","            \n","            preds = self.fc2(self.dropout(output)) # (B,1+L,S)\n","            preds = torch.softmax(preds, dim = -1) # (B,1+L,S)\n","            return preds[:,1:,:] # (B,L,S)\n","        \n","        else:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            \n","            img_features = self.fc1(avg_features) # (B,F) --> (B,E)\n","            caption = torch.ones((img_features.shape[0],1), dtype=torch.long).to(img_features.device) * bos_id # (B,1)\n","            \n","            for i in range(max_len):\n","                cap_embed = self.embed(caption) # (B,L',E)\n","                embed = torch.cat([img_features.unsqueeze(1), cap_embed], dim=1) # (B,1+L',E)\n","                \n","                output, _ = self.rnn(embed) # (B,1+L',H)\n","                \n","                preds = self.fc2(self.dropout(output)) # (B,1+L',S)\n","                preds = torch.softmax(preds, dim = -1) # (B,1+L',S)\n","                preds = torch.argmax(preds[:,-1,:], dim=-1, keepdim=True) # (B,1)\n","                caption = torch.cat([caption, preds], dim=-1) # (B,L'+1)\n","            \n","            return caption # (B,L')\n","        \n","class SAT(nn.Module): # Show, Attend and Tell\n","    def __init__(self, image_encoder, num_tokens, fc_features=1024, embed_dim=256, hidden_size=512, dropout=0.1, freeze_encoder=True):\n","        super().__init__()\n","        self.embed = nn.Embedding(num_tokens, embed_dim)\n","        \n","        self.image_encoder = image_encoder\n","        if freeze_encoder: # The orginal paper freeze the densenet which is pretrained on ImageNet. Suprisingly, the results were very good\n","            for param in self.image_encoder.parameters():\n","                param.requires_grad = False\n","        \n","        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=hidden_size, batch_first=True)\n","        \n","        self.fc1 = nn.Linear(fc_features, embed_dim)\n","        self.fc2 = nn.Linear(hidden_size, num_tokens)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, image, caption=None, caption_length=None, bos_id=1, eos_id=2, pad_id=3, max_len=300):\n","        if caption != None:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            \n","            img_features = self.fc1(avg_features) # (B,F) --> (B,E)\n","            cap_embed = self.embed(caption) # (B,L,E)\n","            embed = torch.cat([img_features.unsqueeze(1), cap_embed], dim=1) # (B,1+L,E)\n","            \n","            output, _ = self.rnn(embed) # (B,1+L,H)\n","            \n","            preds = self.fc2(self.dropout(output)) # (B,1+L,S)\n","            preds = torch.softmax(preds, dim = -1) # (B,1+L,S)\n","            return preds[:,1:,:] # (B,L,S)\n","        \n","        else:\n","            avg_features, wxh_features = self.image_encoder(image) # (B,F), (B,F,W,H)\n","            \n","            img_features = self.fc1(avg_features) # (B,F) --> (B,E)\n","            caption = torch.ones((img_features.shape[0],1), dtype=torch.long).to(img_features.device) * bos_id # (B,1)\n","            \n","            for i in range(max_len):\n","                cap_embed = self.embed(caption) # (B,L',E)\n","                embed = torch.cat([img_features.unsqueeze(1), cap_embed], dim=1) # (B,1+L',E)\n","                \n","                output, _ = self.rnn(embed) # (B,1+L',H)\n","                \n","                preds = self.fc2(self.dropout(output)) # (B,1+L',S)\n","                preds = torch.softmax(preds, dim = -1) # (B,1+L',S)\n","                preds = torch.argmax(preds[:,-1,:], dim=-1, keepdim=True) # (B,1)\n","                caption = torch.cat([caption, preds], dim=-1) # (B,L'+1)\n","            \n","            return caption # (B,L')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:43:05.770938Z","iopub.status.busy":"2023-08-18T16:43:05.769910Z","iopub.status.idle":"2023-08-18T16:43:05.850231Z","shell.execute_reply":"2023-08-18T16:43:05.849121Z","shell.execute_reply.started":"2023-08-18T16:43:05.770897Z"},"trusted":true},"outputs":[],"source":["\n","# --- Transformer Modules ---\n","class MultiheadAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads, dropout=0.0):\n","        super().__init__()\n","        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout)\n","        self.normalize = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, input, query, pad_mask=None, att_mask=None):\n","        input = input.permute(1,0,2) # (V,B,E)\n","        query = query.permute(1,0,2) # (Q,B,E)\n","        embed, att = self.attention(query, input, input, key_padding_mask=pad_mask, attn_mask=att_mask) # (Q,B,E), (B,Q,V)\n","        \n","        embed = self.normalize(embed + query) # (Q,B,E)\n","        embed = embed.permute(1,0,2) # (B,Q,E)\n","        return embed, att # (B,Q,E), (B,Q,V)\n","    \n","class PointwiseFeedForward(nn.Module):\n","    def __init__(self, emb_dim, fwd_dim, dropout=0.0):\n","        super().__init__()\n","        self.fwd_layer = nn.Sequential(\n","            nn.Linear(emb_dim, fwd_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(fwd_dim, emb_dim),\n","        )\n","        self.normalize = nn.LayerNorm(emb_dim)\n","\n","    def forward(self, input):\n","        output = self.fwd_layer(input) # (B,L,E)\n","        output = self.normalize(output + input) # (B,L,E)\n","        return output\n","\n","class TransformerLayer(nn.Module):\n","    def __init__(self, embed_dim, num_heads, fwd_dim, dropout=0.0):\n","        super().__init__()\n","        self.attention = MultiheadAttention(embed_dim, num_heads, dropout)\n","        self.fwd_layer = PointwiseFeedForward(embed_dim, fwd_dim, dropout)\n","\n","    def forward(self, input, pad_mask=None, att_mask=None):\n","        emb, att = self.attention(input,input,pad_mask,att_mask)\n","        emb = self.fwd_layer(emb)\n","        return emb, att\n","\n","class TNN(nn.Module):\n","    def __init__(self, embed_dim, num_heads, fwd_dim, dropout=0.1, num_layers=1,\n","                num_tokens=1, num_posits=1, token_embedding=None, posit_embedding=None):\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(num_tokens, embed_dim) if not token_embedding else token_embedding\n","        self.posit_embedding = nn.Embedding(num_posits, embed_dim) if not posit_embedding else posit_embedding\n","        self.transform = nn.ModuleList([TransformerLayer(embed_dim, num_heads, fwd_dim, dropout) for _ in range(num_layers)])\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, token_index=None, token_embed=None, pad_mask=None, pad_id=-1, att_mask=None):\n","        if token_index != None:\n","            if pad_mask == None:\n","                pad_mask = (token_index == pad_id) # (B,L)\n","            posit_index = torch.arange(token_index.shape[1]).unsqueeze(0).repeat(token_index.shape[0],1).to(token_index.device) # (B,L)\n","            posit_embed = self.posit_embedding(posit_index) # (B,L,E)\n","            token_embed = self.token_embedding(token_index) # (B,L,E)\n","            final_embed = self.dropout(token_embed + posit_embed) # (B,L,E)\n","        elif token_embed != None:\n","            posit_index = torch.arange(token_embed.shape[1]).unsqueeze(0).repeat(token_embed.shape[0],1).to(token_embed.device) # (B,L)\n","            posit_embed = self.posit_embedding(posit_index) # (B,L,E)\n","            final_embed = self.dropout(token_embed + posit_embed) # (B,L,E)\n","        else:\n","            raise ValueError('token_index or token_embed must not be None')\n","\n","        for i in range(len(self.transform)):\n","            final_embed = self.transform[i](final_embed, pad_mask, att_mask)[0]\n","            \n","        return final_embed # (B,L,E)\n","\n","# --- Convolution Modules ---\n","class CNN(nn.Module):\n","    def __init__(self, model, model_type='resnet'):\n","        super().__init__()\n","        if 'res' in model_type.lower(): # resnet, resnet-50, resnest-50, ...\n","            modules = list(model.children())[:-1] # Drop the FC layer\n","            self.feature = nn.Sequential(*modules[:-1])\n","            self.average = modules[-1]\n","        elif 'dense' in model_type.lower(): # densenet, densenet-121, densenet121, ...\n","            modules = list(model.features.children())[:-1]\n","            self.feature = nn.Sequential(*modules)\n","            self.average = nn.AdaptiveAvgPool2d((1, 1))\n","        else:\n","            raise ValueError('Unsupported model_type!')\n","        \n","    def forward(self, input):\n","        wxh_features = self.feature(input) # (B,2048,W,H)\n","        avg_features = self.average(wxh_features) # (B,2048,1,1)\n","        avg_features = avg_features.view(avg_features.shape[0], -1) # (B,2048)\n","        return avg_features, wxh_features\n","\n","class MVCNN(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, input):\n","        img = input[0] # (B,V,C,W,H)\n","        pos = input[1] # (B,V)\n","        B,V,C,W,H = img.shape\n","\n","        img = img.view(B*V,C,W,H)\n","        avg, wxh = self.model(img) # (B*V,F), (B*V,F,W,H)\n","        avg = avg.view(B,V,-1) # (B,V,F)\n","        wxh = wxh.view(B,V,wxh.shape[-3],wxh.shape[-2],wxh.shape[-1]) # (B,V,F,W,H)\n","        \n","        msk = (pos == -1) # (B,V)\n","        msk_wxh = msk.view(B,V,1,1,1).float() # (B,V,1,1,1) * (B,V,F,C,W,H)\n","        msk_avg = msk.view(B,V,1).float() # (B,V,1) * (B,V,F)\n","        wxh = msk_wxh * (-1) + (1-msk_wxh) * wxh\n","        avg = msk_avg * (-1) + (1-msk_avg) * avg\n","\n","        wxh_features = wxh.max(dim=1)[0] # (B,F,W,H)\n","        avg_features = avg.max(dim=1)[0] # (B,F)\n","        return avg_features, wxh_features\n","\n","# --- Main Moduldes ---\n","class Classifier(nn.Module):\n","    def __init__(self, num_topics, num_states, cnn=None, tnn=None,\n","                fc_features=2048, embed_dim=128, num_heads=1, dropout=0.1):\n","        super().__init__()\n","        \n","        # For img & txt embedding and feature extraction\n","        self.cnn = cnn\n","        self.tnn = tnn\n","        self.img_features = nn.Linear(fc_features, num_topics * embed_dim) if cnn != None else None\n","        self.txt_features = MultiheadAttention(embed_dim, num_heads, dropout) if tnn != None else None\n","        \n","        # For classification\n","        self.topic_embedding = nn.Embedding(num_topics, embed_dim)\n","        self.state_embedding = nn.Embedding(num_states, embed_dim)\n","        self.attention = MultiheadAttention(embed_dim, num_heads)\n","        \n","        # Some constants\n","        self.num_topics = num_topics\n","        self.num_states = num_states\n","        self.dropout = nn.Dropout(dropout)\n","        self.normalize = nn.LayerNorm(embed_dim)\n","\n","    def forward(self, img=None, txt=None, lbl=None, txt_embed=None, pad_mask=None, pad_id=3, threshold=0.5, get_embed=False, get_txt_att=False):\n","        # --- Get img and txt features ---\n","        if img != None: # (B,C,W,H) or ((B,V,C,W,H), (B,V))\n","            img_features, wxh_features = self.cnn(img) # (B,F), (B,F,W,H)\n","            img_features = self.dropout(img_features) # (B,F)\n","            \n","        if txt != None:\n","            if pad_id >= 0 and pad_mask == None:\n","                pad_mask = (txt == pad_id)\n","            txt_features = self.tnn(token_index=txt, pad_mask=pad_mask) # (B,L,E)\n","        \n","        elif txt_embed != None:\n","            txt_features = self.tnn(token_embed=txt_embed, pad_mask=pad_mask) # (B,L,E)\n","\n","        # --- Fuse img and txt features ---\n","        if img != None and (txt != None or txt_embed != None):\n","            topic_index = torch.arange(self.num_topics).unsqueeze(0).repeat(img_features.shape[0],1).to(img_features.device) # (B,T)\n","            state_index = torch.arange(self.num_states).unsqueeze(0).repeat(img_features.shape[0],1).to(img_features.device) # (B,C)\n","            topic_embed = self.topic_embedding(topic_index) # (B,T,E)\n","            state_embed = self.state_embedding(state_index) # (B,C,E)\n","            \n","            img_features = self.img_features(img_features).view(img_features.shape[0], self.num_topics, -1) # (B,F) --> (B,T*E) --> (B,T,E)   \n","            txt_features, txt_attention = self.txt_features(txt_features, topic_embed, pad_mask) # (B,T,E), (B,T,L)\n","            final_embed = self.normalize(img_features + txt_features) # (B,T,E)\n","            \n","        elif img != None:\n","            topic_index = torch.arange(self.num_topics).unsqueeze(0).repeat(img_features.shape[0],1).to(img_features.device) # (B,T)\n","            state_index = torch.arange(self.num_states).unsqueeze(0).repeat(img_features.shape[0],1).to(img_features.device) # (B,C)\n","            topic_embed = self.topic_embedding(topic_index) # (B,T,E)\n","            state_embed = self.state_embedding(state_index) # (B,C,E)\n","\n","            img_features = self.img_features(img_features).view(img_features.shape[0], self.num_topics, -1) # (B,F) --> (B,T*E) --> (B,T,E)   \n","            final_embed = img_features # (B,T,E)\n","            \n","        elif txt != None or txt_embed != None:\n","            topic_index = torch.arange(self.num_topics).unsqueeze(0).repeat(txt_features.shape[0],1).to(txt_features.device) # (B,T)\n","            state_index = torch.arange(self.num_states).unsqueeze(0).repeat(txt_features.shape[0],1).to(txt_features.device) # (B,C)\n","            topic_embed = self.topic_embedding(topic_index) # (B,T,E)\n","            state_embed = self.state_embedding(state_index) # (B,C,E)\n","\n","            txt_features, txt_attention = self.txt_features(txt_features, topic_embed, pad_mask) # (B,T,E), (B,T,L)\n","            final_embed = txt_features # (B,T,E)\n","            \n","        else:\n","            raise ValueError('img and (txt or txt_embed) must not be all none')\n","        \n","        # Classifier output\n","        emb, att = self.attention(state_embed, final_embed) # (B,T,E), (B,T,C)\n","        \n","        if lbl != None: # Teacher forcing\n","            emb = self.state_embedding(lbl) # (B,T,E)\n","        else:\n","            emb = self.state_embedding((att[:,:,1] > threshold).long()) # (B,T,E)\n","            \n","        if get_embed:\n","            return att, final_embed + emb # (B,T,C), (B,T,E)\n","        elif get_txt_att and (txt != None or txt_embed != None):\n","            return att, txt_attention # (B,T,C), (B,T,L)\n","        else:\n","            return att # (B,T,C)\n","\n","class Generator(nn.Module):\n","    def __init__(self, num_tokens, num_posits, embed_dim=128, num_heads=1, fwd_dim=256, dropout=0.1, num_layers=12):\n","        super().__init__()\n","        self.token_embedding = nn.Embedding(num_tokens, embed_dim)\n","        self.posit_embedding = nn.Embedding(num_posits, embed_dim)\n","        self.transform = nn.ModuleList([TransformerLayer(embed_dim, num_heads, fwd_dim, dropout) for _ in range(num_layers)])\n","        self.attention = MultiheadAttention(embed_dim, num_heads)\n","        self.num_tokens = num_tokens\n","        self.num_posits = num_posits\n","        \n","    def forward(self, source_embed, token_index=None, source_pad_mask=None, target_pad_mask=None, max_len=300, top_k=1, bos_id=1, pad_id=3, mode='eye'):\n","        if token_index != None: # --- Training/Testing Phase ---\n","            # Adding token embedding and posititional embedding.\n","            posit_index = torch.arange(token_index.shape[1]).unsqueeze(0).repeat(token_index.shape[0],1).to(token_index.device) # (1,L) --> (B,L)\n","            posit_embed = self.posit_embedding(posit_index) # (B,L,E)\n","            token_embed = self.token_embedding(token_index) # (B,L,E)\n","            target_embed = token_embed + posit_embed # (B,L,E)\n","            \n","            # Make embedding, attention mask, pad mask for Transformer Decoder\n","            final_embed = torch.cat([source_embed,target_embed], dim=1) # (B,T+L,E)\n","            if source_pad_mask == None:\n","                source_pad_mask = torch.zeros((source_embed.shape[0],source_embed.shape[1]),device=source_embed.device).bool() # (B,T)\n","            if target_pad_mask == None:\n","                target_pad_mask = torch.zeros((target_embed.shape[0],target_embed.shape[1]),device=target_embed.device).bool() # (B,L)\n","            pad_mask = torch.cat([source_pad_mask,target_pad_mask], dim=1) # (B,T+L)\n","            att_mask = self.generate_square_subsequent_mask_with_source(source_embed.shape[1], target_embed.shape[1], mode).to(final_embed.device) # (T+L,T+L)\n","\n","            # Transformer Decoder\n","            for i in range(len(self.transform)):\n","                final_embed = self.transform[i](final_embed,pad_mask,att_mask)[0]\n","\n","            # Make prediction for next tokens\n","            token_index = torch.arange(self.num_tokens).unsqueeze(0).repeat(token_index.shape[0],1).to(token_index.device) # (1,K) --> (B,K)\n","            token_embed = self.token_embedding(token_index) # (B,K,E)\n","            emb, att = self.attention(token_embed,final_embed) # (B,T+L,E), (B,T+L,K)\n","            \n","            # Truncate results from source_embed\n","            emb = emb[:,source_embed.shape[1]:,:] # (B,L,E)\n","            att = att[:,source_embed.shape[1]:,:] # (B,L,K)\n","            return att, emb\n","        \n","        else: # --- Inference Phase ---\n","            return self.infer(source_embed, source_pad_mask, max_len, top_k, bos_id, pad_id)\n","\n","    def infer(self, source_embed, source_pad_mask=None, max_len=100, top_k=1, bos_id=1, pad_id=3):\n","        outputs = torch.ones((top_k, source_embed.shape[0], 1), dtype=torch.long).to(source_embed.device) * bos_id # (K,B,1) <s>\n","        scores = torch.zeros((top_k, source_embed.shape[0]), dtype=torch.float32).to(source_embed.device) # (K,B)\n","\n","        for _ in range(1,max_len):\n","            possible_outputs = []\n","            possible_scores = []\n","\n","            for k in range(top_k):\n","                output = outputs[k] # (B,L)\n","                score = scores[k] # (B)\n","                \n","                att, emb = self.forward(source_embed, output, source_pad_mask=source_pad_mask, target_pad_mask=(output == pad_id))\n","                val, idx = torch.topk(att[:,-1,:], top_k) # (B,K)\n","                log_val = -torch.log(val) # (B,K)\n","                \n","                for i in range(top_k):\n","                    new_output = torch.cat([output, idx[:,i].view(-1,1)], dim=-1) # (B,L+1)\n","                    new_score = score + log_val[:,i].view(-1) # (B)\n","                    possible_outputs.append(new_output.unsqueeze(0)) # (1,B,L+1)\n","                    possible_scores.append(new_score.unsqueeze(0)) # (1,B)\n","            \n","            possible_outputs = torch.cat(possible_outputs, dim=0) # (K^2,B,L+1)\n","            possible_scores = torch.cat(possible_scores, dim=0) # (K^2,B)\n","\n","            # Pruning the solutions\n","            val, idx = torch.topk(possible_scores, top_k, dim=0) # (K,B)\n","            col_idx = torch.arange(idx.shape[1], device=idx.device).unsqueeze(0).repeat(idx.shape[0],1) # (K,B)\n","            outputs = possible_outputs[idx,col_idx] # (K,B,L+1)\n","            scores = possible_scores[idx,col_idx] # (K,B)\n","\n","        val, idx = torch.topk(scores, 1, dim=0) # (1,B)\n","        col_idx = torch.arange(idx.shape[1], device=idx.device).unsqueeze(0).repeat(idx.shape[0],1) # (K,B)\n","        output = outputs[idx,col_idx] # (1,B,L)\n","        score = scores[idx,col_idx] # (1,B)\n","        return output.squeeze(0) # (B,L)\n","\n","    def generate_square_subsequent_mask_with_source(self, src_sz, tgt_sz, mode='eye'):\n","        mask = self.generate_square_subsequent_mask(src_sz + tgt_sz)\n","        if mode == 'one': # model can look at surrounding positions of the current index ith\n","            mask[:src_sz, :src_sz] = self.generate_square_mask(src_sz)\n","        elif mode == 'eye': # model can only look at the current index ith\n","            mask[:src_sz, :src_sz] = self.generate_square_identity_mask(src_sz)\n","        else: # model can look at surrounding positions of the current index ith with some patterns\n","            raise ValueError('Mode must be \"one\" or \"eye\".')\n","        mask[src_sz:, src_sz:] = self.generate_square_subsequent_mask(tgt_sz)\n","        return mask\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def generate_square_identity_mask(self, sz):\n","        mask = (torch.eye(sz) == 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask \n","\n","    def generate_square_mask(self, sz):\n","        mask = (torch.ones(sz,sz) == 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","# --- Full Models ---\n","class ClsGen(nn.Module):\n","    def __init__(self, classifier, generator, num_topics, embed_dim):\n","        super().__init__()\n","        self.classifier = classifier\n","        self.generator = generator\n","        self.label_embedding = nn.Embedding(num_topics, embed_dim)\n","\n","    def forward(self, image, history=None, caption=None, label=None, threshold=0.15, bos_id=1, eos_id=2, pad_id=3, max_len=300, get_emb=False):\n","        label = label.long() if label != None else label\n","        img_mlc, img_emb = self.classifier(img=image, txt=history, lbl=label, threshold=threshold, pad_id=pad_id, get_embed=True) # (B,T,C), (B,T,E)\n","        lbl_idx = torch.arange(img_emb.shape[1]).unsqueeze(0).repeat(img_emb.shape[0],1).to(img_emb.device) # (B,T)\n","        lbl_emb = self.label_embedding(lbl_idx) # (B,T,E)\n","        \n","        if caption != None:\n","            src_emb = img_emb + lbl_emb\n","            pad_mask = (caption == pad_id)\n","            cap_gen, cap_emb = self.generator(source_embed=src_emb, token_index=caption, target_pad_mask=pad_mask) # (B,L,S), (B,L,E)\n","            if get_emb:\n","                return cap_gen, img_mlc, cap_emb\n","            else:\n","                return cap_gen, img_mlc\n","        else:\n","            src_emb = img_emb + lbl_emb\n","            cap_gen = self.generator(source_embed=src_emb, token_index=caption, max_len=max_len, bos_id=bos_id, pad_id=pad_id) # (B,L,S)\n","            return cap_gen, img_mlc\n","\n","class ClsGenInt(nn.Module):\n","    def __init__(self, clsgen, interpreter, freeze_evaluator=True):\n","        super().__init__()\n","        self.clsgen = clsgen\n","        self.interpreter = interpreter\n","            \n","        # Freeze evaluator's paramters\n","        if freeze_evaluator:\n","            for param in self.interpreter.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, image, history=None, caption=None, label=None, threshold=0.15, bos_id=1, eos_id=2, pad_id=3, max_len=300):        \n","        if caption != None:\n","            pad_mask = (caption == pad_id)\n","            cap_gen, img_mlc, cap_emb = self.clsgen(image, history, caption, label, threshold, bos_id, eos_id, pad_id, max_len, True)\n","            cap_mlc = self.interpreter(txt_embed=cap_emb, pad_mask=pad_mask)\n","            return cap_gen, img_mlc, cap_mlc\n","        else:\n","            return self.clsgen(image, history, caption, label, threshold, bos_id, eos_id, pad_id, max_len, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T16:57:25.788859Z","iopub.status.busy":"2023-08-18T16:57:25.788291Z","iopub.status.idle":"2023-08-18T16:57:25.807164Z","shell.execute_reply":"2023-08-18T16:57:25.806036Z","shell.execute_reply.started":"2023-08-18T16:57:25.788817Z"},"trusted":true},"outputs":[],"source":["# --- Helper Functions ---\n","def find_optimal_cutoff(target, predicted):\n","    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n","    gmeans = np.sqrt(tpr * (1-fpr))\n","    ix = np.argmax(gmeans)\n","    return threshold[ix]\n","\n","def infer(data_loader, model, device='cpu', threshold=None):\n","    model.eval()\n","    outputs = []\n","    targets = []\n","\n","    with torch.no_grad():\n","        prog_bar = tqdm(data_loader)\n","        for i, (source, target) in enumerate(prog_bar):\n","            source = data_to_device(source, device)\n","            target = data_to_device(target, device)\n","\n","            # Use single input if there is no clinical history\n","            if threshold != None:\n","                output = model(image=source[0], history=source[3], threshold=threshold)\n","                # output = model(image=source[0], threshold=threshold)\n","                # output = model(image=source[0], history=source[3], label=source[2])\n","                # output = model(image=source[0], label=source[2])\n","            else:\n","                # output = model(source[0], source[1])\n","                output = model(source[0])\n","                \n","            outputs.append(data_to_device(output))\n","            targets.append(data_to_device(target))\n","\n","        outputs = data_concatenate(outputs)\n","        targets = data_concatenate(targets)\n","    \n","    return outputs, targets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load(path, model, optimizer=None, scheduler=None):\n","\tcheckpoint = torch.load(path,map_location=torch.device('cpu'))\n","\tprint(checkpoint.keys())\n","\t# --- Model Statistics ---\n","\tepoch = checkpoint['epoch']\n","\tstats = checkpoint['stats']\n","\t# --- Model Parameters ---\n","\tmodel.load_state_dict(checkpoint['model_state_dict'])\n","\tif optimizer != None:\n","\t\ttry:\n","\t\t\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\t\texcept: # Input optimizer doesn't fit the checkpoint one --> should be ignored\n","\t\t\tprint('Cannot load the optimizer')\n","\tif scheduler != None:\n","\t\ttry:\n","\t\t\tscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\t\texcept: # Input scheduler doesn't fit the checkpoint one --> should be ignored\n","\t\t\tprint('Cannot load the scheduler')\n","\treturn epoch, stats"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T17:12:21.299159Z","iopub.status.busy":"2023-08-18T17:12:21.298776Z"},"trusted":true},"outputs":[],"source":["\n","# --- Hyperparameters ---\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","torch.set_num_threads(1)\n","torch.manual_seed(seed=123)\n","\n","RELOAD = True # True / False\n","PHASE = 'INFER' # TRAIN / TEST / INFER\n","DATASET_NAME = 'NLMCXR' # NIHCXR / NLMCXR / MIMIC \n","BACKBONE_NAME = 'DenseNet121' # ResNeSt50 / ResNet50 / DenseNet121\n","MODEL_NAME = 'ClsGenInt' # ClsGen / ClsGenInt / VisualTransformer / GumbelTransformer\n","\n","if DATASET_NAME == 'MIMIC':\n","    EPOCHS = 50 # Start overfitting after 20 epochs\n","    BATCH_SIZE = 8 if PHASE == 'TRAIN' else 64 # 128 # Fit 4 GPUs\n","    MILESTONES = [25] # Reduce LR by 10 after reaching milestone epochs\n","    \n","elif DATASET_NAME == 'NLMCXR':\n","    EPOCHS = 50 # Start overfitting after 20 epochs\n","    BATCH_SIZE = 8 if PHASE == 'TRAIN' else 16 # Fit 4 GPUs\n","    MILESTONES = [25] # Reduce LR by 10 after reaching milestone epochs\n","    \n","else:\n","    raise ValueError('Invalid DATASET_NAME')\n","\n","if __name__ == \"__main__\":\n","    # --- Choose Inputs/Outputs\n","    if MODEL_NAME in ['ClsGen', 'ClsGenInt']:\n","        SOURCES = ['image','caption','label','history']\n","        TARGETS = ['caption','label']\n","        KW_SRC = ['image','caption','label','history']\n","        KW_TGT = None\n","        KW_OUT = None\n","                \n","    elif MODEL_NAME == 'VisualTransformer':\n","        SOURCES = ['image','caption']\n","        TARGETS = ['caption']# ,'label']\n","        KW_SRC = ['image','caption'] # kwargs of Classifier\n","        KW_TGT = None\n","        KW_OUT = None\n","        \n","    elif MODEL_NAME == 'GumbelTransformer':\n","        SOURCES = ['image','caption','caption_length']\n","        TARGETS = ['caption','label']\n","        KW_SRC = ['image','caption','caption_length'] # kwargs of Classifier\n","        KW_TGT = None\n","        KW_OUT = None\n","        \n","    else:\n","        raise ValueError('Invalid BACKBONE_NAME')\n","        \n","    # --- Choose a Dataset ---\n","    if DATASET_NAME == 'NLMCXR':\n","        INPUT_SIZE = (256,256)\n","        MAX_VIEWS = 2\n","        NUM_LABELS = 114\n","        NUM_CLASSES = 2\n","\n","        dataset = NLMCXR('/kaggle/input/', INPUT_SIZE, view_pos=['AP','PA','LATERAL'], max_views=MAX_VIEWS, sources=SOURCES, targets=TARGETS)\n","        train_data, val_data, test_data = dataset.get_subsets(seed=123)\n","        \n","        VOCAB_SIZE = len(dataset.vocab)\n","        POSIT_SIZE = dataset.max_len\n","        COMMENT = 'MaxView{}_NumLabel{}_{}History'.format(MAX_VIEWS, NUM_LABELS, 'No' if 'history' not in SOURCES else '')\n","        \n","    else:\n","        raise ValueError('Invalid DATASET_NAME')\n","\n","    # --- Choose a Backbone --- \n","    if BACKBONE_NAME == 'ResNeSt50':\n","        torch.hub.list('zhanghang1989/ResNeSt', force_reload=True)\n","        backbone = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n","        FC_FEATURES = 2048\n","        \n","    elif BACKBONE_NAME == 'ResNet50':\n","        backbone = models.resnet50(pretrained=True)\n","        FC_FEATURES = 2048\n","        \n","    elif BACKBONE_NAME == 'DenseNet121':\n","        backbone = torch.hub.load('pytorch/vision:v0.5.0', 'densenet121', pretrained=True)\n","        FC_FEATURES = 1024\n","        \n","    else:\n","        raise ValueError('Invalid BACKBONE_NAME')\n","\n","    # --- Choose a Model ---\n","    if MODEL_NAME == 'ClsGen':\n","        LR = 3e-4 # Fastest LR\n","        WD = 1e-2 # Avoid overfitting with L2 regularization\n","        DROPOUT = 0.1 # Avoid overfitting\n","        NUM_EMBEDS = 256\n","        FWD_DIM = 256\n","        \n","        NUM_HEADS = 8\n","        NUM_LAYERS = 1\n","        \n","        cnn = CNN(backbone, BACKBONE_NAME)\n","        cnn = MVCNN(cnn)\n","        tnn = TNN(embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, dropout=DROPOUT, num_layers=NUM_LAYERS, num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE)\n","        \n","        # Not enough memory to run 8 heads and 12 layers, instead 1 head is enough\n","        NUM_HEADS = 1\n","        NUM_LAYERS = 12\n","        \n","        cls_model = Classifier(num_topics=NUM_LABELS, num_states=NUM_CLASSES, cnn=cnn, tnn=tnn, fc_features=FC_FEATURES, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, dropout=DROPOUT)\n","        gen_model = Generator(num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, dropout=DROPOUT, num_layers=NUM_LAYERS)\n","        \n","        model = ClsGen(cls_model, gen_model, NUM_LABELS, NUM_EMBEDS)\n","        criterion = CELossTotal(ignore_index=3)\n","        \n","    elif MODEL_NAME == 'ClsGenInt':\n","        LR = 3e-5 # Slower LR to fine-tune the model (Open-I)\n","        # LR = 3e-6 # Slower LR to fine-tune the model (MIMIC)\n","        WD = 1e-2 # Avoid overfitting with L2 regularization\n","        DROPOUT = 0.1 # Avoid overfitting\n","        NUM_EMBEDS = 256\n","        FWD_DIM = 256\n","        \n","        NUM_HEADS = 8\n","        NUM_LAYERS = 1\n","        \n","        cnn = CNN(backbone, BACKBONE_NAME)\n","        cnn = MVCNN(cnn)\n","        tnn = TNN(embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, dropout=DROPOUT, num_layers=NUM_LAYERS, num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE)\n","        \n","        # Not enough memory to run 8 heads and 12 layers, instead 1 head is enough\n","        NUM_HEADS = 1\n","        NUM_LAYERS = 12\n","        \n","        cls_model = Classifier(num_topics=NUM_LABELS, num_states=NUM_CLASSES, cnn=cnn, tnn=tnn, fc_features=FC_FEATURES, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, dropout=DROPOUT)\n","        gen_model = Generator(num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, dropout=DROPOUT, num_layers=NUM_LAYERS)\n","        \n","        clsgen_model = ClsGen(cls_model, gen_model, NUM_LABELS, NUM_EMBEDS)\n","        clsgen_model = nn.DataParallel(clsgen_model)\n","        \n","        if not RELOAD:\n","            checkpoint_path_from = 'checkpoints/{}_ClsGen_{}_{}.pt'.format(DATASET_NAME, BACKBONE_NAME, COMMENT)\n","            last_epoch, (best_metric, test_metric) = load(checkpoint_path_from, clsgen_model)\n","            print('Reload From: {} | Last Epoch: {} | Validation Metric: {} | Test Metric: {}'.format(checkpoint_path_from, last_epoch, best_metric, test_metric))\n","        \n","        # Initialize the Interpreter module\n","        NUM_HEADS = 8\n","        NUM_LAYERS = 1\n","        \n","        tnn = TNN(embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, dropout=DROPOUT, num_layers=NUM_LAYERS, num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE)\n","        int_model = Classifier(num_topics=NUM_LABELS, num_states=NUM_CLASSES, cnn=None, tnn=tnn, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, dropout=DROPOUT)\n","        int_model = nn.DataParallel(int_model)\n","        \n","        if not RELOAD:\n","            checkpoint_path_from = 'checkpoints/{}_Transformer_MaxView2_NumLabel{}.pt'.format(DATASET_NAME, NUM_LABELS)\n","            last_epoch, (best_metric, test_metric) = load(checkpoint_path_from, int_model)\n","            print('Reload From: {} | Last Epoch: {} | Validation Metric: {} | Test Metric: {}'.format(checkpoint_path_from, last_epoch, best_metric, test_metric))\n","        \n","        model = ClsGenInt(clsgen_model.module.cpu(), int_model.module.cpu(), freeze_evaluator=True)\n","        criterion = CELossTotalEval(ignore_index=3)\n","        \n","    elif MODEL_NAME == 'VisualTransformer':\n","        # Clinical Coherent X-ray Report (Justin et. al.) - No Fine-tune\n","        LR = 5e-5\n","        WD = 1e-2 # Avoid overfitting with L2 regularization\n","        DROPOUT = 0.1 # Avoid overfitting\n","        NUM_EMBEDS = 256\n","        NUM_HEADS = 8\n","        FWD_DIM = 4096\n","        NUM_LAYERS_ENC = 1\n","        NUM_LAYERS_DEC = 6\n","        \n","        cnn = CNN(backbone, BACKBONE_NAME)\n","        cnn = MVCNN(cnn)\n","        model = Transformer(image_encoder=cnn, num_tokens=VOCAB_SIZE, num_posits=POSIT_SIZE, \n","                            fc_features=FC_FEATURES, embed_dim=NUM_EMBEDS, num_heads=NUM_HEADS, fwd_dim=FWD_DIM, \n","                            dropout=DROPOUT, num_layers_enc=NUM_LAYERS_ENC, num_layers_dec=NUM_LAYERS_DEC, freeze_encoder=True)\n","        criterion = CELossShift(ignore_index=3)\n","    else:\n","        raise ValueError('Invalid MODEL_NAME')\n","    \n","    # --- Main program ---\n","    train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)\n","    val_loader = data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n","    test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n","\n","    model = nn.DataParallel(model)\n","    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WD)\n","    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES)\n","\n","    print('Total Parameters:', sum(p.numel() for p in model.parameters()))\n","    \n","    last_epoch = -1\n","    best_metric = 1e9\n","\n","    checkpoint_path_from = '/kaggle/input/nlm-torch-model/NLMCXR_ClsGenInt_DenseNet121_MaxView2_NumLabel114_History.pt' #'/kaggle/input/torch-models/{}_{}_{}_{}.pt'.format(DATASET_NAME,MODEL_NAME,BACKBONE_NAME,COMMENT)\n","    checkpoint_path_to = '/kaggle/working/checkpoints/{}_{}_{}_{}.pt'.format(DATASET_NAME,MODEL_NAME,BACKBONE_NAME,COMMENT)\n","    \n","    if RELOAD:\n","        last_epoch, (best_metric, test_metric) = load(checkpoint_path_from, model, optimizer, scheduler) # Reload\n","        # last_epoch, (best_metric, test_metric) = load(checkpoint_path_from, model) # Fine-tune\n","        print('Reload From: {} | Last Epoch: {} | Validation Metric: {} | Test Metric: {}'.format(checkpoint_path_from, last_epoch, best_metric, test_metric))\n","\n","    if PHASE == 'TRAIN':\n","        scaler = torch.cuda.amp.GradScaler()\n","        \n","        for epoch in range(last_epoch+1, EPOCHS):\n","            val_loss = -1\n","            test_loss = -1\n","            print('Epoch:', epoch)\n","            save(checkpoint_path_to, model, optimizer, scheduler, epoch, (val_loss, test_loss))\n","            train_loss = train(train_loader, model, optimizer, criterion, device='cuda', kw_src=KW_SRC, kw_tgt=KW_TGT, kw_out=KW_OUT, scaler=scaler)\n","            val_loss = test(val_loader, model, criterion, device='cuda', kw_src=KW_SRC, kw_tgt=KW_TGT, kw_out=KW_OUT, return_results=False)\n","            test_loss = test(test_loader, model, criterion, device='cuda', kw_src=KW_SRC, kw_tgt=KW_TGT, kw_out=KW_OUT, return_results=False)\n","            \n","            scheduler.step()\n","            \n","            if best_metric > val_loss:\n","                best_metric = val_loss\n","                save(checkpoint_path_to, model, optimizer, scheduler, epoch, (val_loss, test_loss))\n","                print('New Best Metric: {}'.format(best_metric)) \n","                print('Saved To:', checkpoint_path_to)\n","    elif PHASE == 'TEST':\n","        # Output the file list for inspection\n","        out_file_img = open('outputs/{}_{}_{}_{}_Img.txt'.format(DATASET_NAME, MODEL_NAME, BACKBONE_NAME, COMMENT), 'w')\n","        for i in range(len(test_data.idx_pidsid)):\n","            out_file_img.write(test_data.idx_pidsid[i][0] + ' ' + test_data.idx_pidsid[i][1] + '\\n')\n","            \n","        \n","    elif PHASE == 'INFER':\n","        txt_test_outputs, txt_test_targets = infer(test_loader, model, device='cpu', threshold=0.25)\n","        gen_outputs = txt_test_outputs[0]\n","        gen_targets = txt_test_targets[0]\n","        \n","        out_file_ref = open('/kaggle/working/x_{}_{}_{}_{}_Ref.txt'.format(DATASET_NAME, MODEL_NAME, BACKBONE_NAME, COMMENT), 'w')\n","        out_file_hyp = open('/kaggle/working/x_{}_{}_{}_{}_Hyp.txt'.format(DATASET_NAME, MODEL_NAME, BACKBONE_NAME, COMMENT), 'w')\n","        out_file_lbl = open('/kaggle/working/x_{}_{}_{}_{}_Lbl.txt'.format(DATASET_NAME, MODEL_NAME, BACKBONE_NAME, COMMENT), 'w')\n","        \n","        for i in range(len(gen_outputs)):\n","            candidate = ''\n","            for j in range(len(gen_outputs[i])):\n","                tok = dataset.vocab.id_to_piece(int(gen_outputs[i,j]))\n","                if tok == '</s>':\n","                    break # Manually stop generating token after </s> is reached\n","                elif tok == '<s>':\n","                    continue\n","                elif tok == '▁': # space\n","                    if len(candidate) and candidate[-1] != ' ':\n","                        candidate += ' '\n","                elif tok in [',', '.', '-', ':']: # or not tok.isalpha():\n","                    if len(candidate) and candidate[-1] != ' ':\n","                        candidate += ' ' + tok + ' ' \n","                    else:\n","                        candidate += tok + ' '\n","                else: # letter\n","                    candidate += tok       \n","            out_file_hyp.write(candidate + '\\n')\n","            \n","            reference = ''\n","            for j in range(len(gen_targets[i])):\n","                tok = dataset.vocab.id_to_piece(int(gen_targets[i,j]))\n","                if tok == '</s>':\n","                    break\n","                elif tok == '<s>':\n","                    continue\n","                elif tok == '▁': # space\n","                    if len(reference) and reference[-1] != ' ':\n","                        reference += ' '\n","                elif tok in [',', '.', '-', ':']: # or not tok.isalpha():\n","                    if len(reference) and reference[-1] != ' ':\n","                        reference += ' ' + tok + ' ' \n","                    else:\n","                        reference += tok + ' '\n","                else: # letter\n","                    reference += tok    \n","            out_file_ref.write(reference + '\\n')\n","\n","        for i in tqdm(range(len(test_data))):\n","            target = test_data[i][1] # caption, label\n","            out_file_lbl.write(' '.join(map(str,target[1])) + '\\n')\n","                \n","    else:\n","        raise ValueError('Invalid PHASE')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
